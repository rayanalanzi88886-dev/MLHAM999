// services/unified-ai.ts - Ø®Ø¯Ù…Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ÙˆØ­Ø¯Ø© ÙˆÙ…Ø­Ø³Ù‘Ù†Ø©

import Anthropic from '@anthropic-ai/sdk';
import { Expert, ChatMessage, ChatAttachment } from '../types-hybrid';
import { MODEL_COSTS } from '../data/experts-hybrid';

// ===== Ø§Ù„ÙˆØ§Ø¬Ù‡Ø§Øª =====
export interface AIResponse {
  content: string;
  modelUsed: string;
  cost: number;
  tokensUsed: {
    input: number;
    output: number;
  };
  provider: 'claude' | 'gemini';
  cached?: boolean;
}

// ===== ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ =====
const anthropic = new Anthropic({
  apiKey: import.meta.env.VITE_ANTHROPIC_API_KEY,
  dangerouslyAllowBrowser: true
});

// ===== Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ø°ÙƒÙŠ =====
class ResponseCache {
  private cache: Map<string, { response: string; timestamp: number; cost: number }>;
  private readonly TTL = 3600000; // Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©

  constructor() {
    this.cache = new Map();
    this.loadFromStorage();
  }

  private loadFromStorage() {
    try {
      const stored = localStorage.getItem('ai_response_cache');
      if (stored) {
        const data = JSON.parse(stored);
        this.cache = new Map(Object.entries(data));
      }
    } catch (error) {
      console.warn('Failed to load cache:', error);
    }
  }

  private saveToStorage() {
    try {
      const data = Object.fromEntries(this.cache.entries());
      localStorage.setItem('ai_response_cache', JSON.stringify(data));
    } catch (error) {
      console.warn('Failed to save cache:', error);
    }
  }

  private generateKey(expertId: string, userMessage: string): string {
    // Normalize the message
    const normalized = userMessage.trim().toLowerCase()
      .replace(/[ØŸ?!.ØŒ,]/g, '') // Remove punctuation
      .replace(/\s+/g, ' '); // Normalize spaces
    
    return `${expertId}:${normalized}`;
  }

  get(expertId: string, userMessage: string): { response: string; cost: number } | null {
    const key = this.generateKey(expertId, userMessage);
    const cached = this.cache.get(key);
    
    if (!cached) return null;
    
    // Check if expired
    if (Date.now() - cached.timestamp > this.TTL) {
      this.cache.delete(key);
      this.saveToStorage();
      return null;
    }
    
    return { response: cached.response, cost: cached.cost };
  }

  set(expertId: string, userMessage: string, response: string, cost: number): void {
    const key = this.generateKey(expertId, userMessage);
    this.cache.set(key, { 
      response, 
      timestamp: Date.now(),
      cost 
    });
    this.saveToStorage();
  }

  cleanup(): void {
    const now = Date.now();
    let cleaned = 0;
    
    for (const [key, value] of this.cache.entries()) {
      if (now - value.timestamp > this.TTL) {
        this.cache.delete(key);
        cleaned++;
      }
    }
    
    if (cleaned > 0) {
      this.saveToStorage();
      console.log(`ğŸ§¹ Cleaned ${cleaned} expired cache entries`);
    }
  }

  getStats() {
    return {
      size: this.cache.size,
      oldestEntry: Math.min(...Array.from(this.cache.values()).map(v => v.timestamp)),
    };
  }
}

const responseCache = new ResponseCache();

// Cleanup cache every 30 minutes
setInterval(() => responseCache.cleanup(), 1800000);

// ===== ØªØªØ¨Ø¹ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… =====
export class UsageTracker {
  private totalCost: number = 0;
  private callCount: number = 0;
  private modelUsage: Record<string, number> = {};
  private providerUsage: Record<string, number> = {};
  private cacheHits: number = 0;
  private cacheMisses: number = 0;

  constructor() {
    this.loadFromStorage();
  }

  private loadFromStorage() {
    try {
      const stored = localStorage.getItem('usage_stats');
      if (stored) {
        const data = JSON.parse(stored);
        Object.assign(this, data);
      }
    } catch (error) {
      console.warn('Failed to load usage stats:', error);
    }
  }

  private saveToStorage() {
    try {
      const data = {
        totalCost: this.totalCost,
        callCount: this.callCount,
        modelUsage: this.modelUsage,
        providerUsage: this.providerUsage,
        cacheHits: this.cacheHits,
        cacheMisses: this.cacheMisses,
      };
      localStorage.setItem('usage_stats', JSON.stringify(data));
    } catch (error) {
      console.warn('Failed to save usage stats:', error);
    }
  }

  trackCall(response: AIResponse): void {
    this.callCount++;
    this.totalCost += response.cost;
    
    if (response.cached) {
      this.cacheHits++;
    } else {
      this.cacheMisses++;
      this.modelUsage[response.modelUsed] = (this.modelUsage[response.modelUsed] || 0) + 1;
      this.providerUsage[response.provider] = (this.providerUsage[response.provider] || 0) + 1;
    }
    
    this.saveToStorage();
  }

  getStats() {
    const cacheHitRate = this.callCount > 0 
      ? (this.cacheHits / this.callCount) * 100 
      : 0;

    return {
      totalCalls: this.callCount,
      totalCost: this.totalCost,
      averageCost: this.callCount > 0 ? this.totalCost / this.callCount : 0,
      modelUsage: this.modelUsage,
      providerUsage: this.providerUsage,
      cacheHitRate: cacheHitRate,
      cacheHits: this.cacheHits,
      cacheMisses: this.cacheMisses,
      costSavedByCache: this.cacheHits * 0.015 // Average Haiku cost
    };
  }

  reset(): void {
    this.totalCost = 0;
    this.callCount = 0;
    this.modelUsage = {};
    this.providerUsage = {};
    this.cacheHits = 0;
    this.cacheMisses = 0;
    this.saveToStorage();
  }
}

export const usageTracker = new UsageTracker();

// ===== Claude API =====
async function callClaude(
  messages: ChatMessage[],
  expert: Expert,
  attachments: ChatAttachment[]
): Promise<AIResponse> {
  
  // Select model based on expert configuration
  const modelName = expert.recommendedModel === 'haiku' 
    ? 'claude-3-haiku-20240307' 
    : expert.recommendedModel === 'sonnet'
      ? 'claude-3-5-sonnet-20240620'
      : 'claude-3-opus-20240229';

  const modelConfig = MODEL_COSTS[expert.recommendedModel as keyof typeof MODEL_COSTS] || MODEL_COSTS['haiku'];

  // Convert messages to Anthropic format
  const anthropicMessages: Anthropic.MessageParam[] = messages
    .filter(msg => msg.role !== 'system')
    .map(msg => ({
      role: msg.role === 'model' ? 'assistant' : msg.role as 'user' | 'assistant',
      content: msg.content
    }));

  try {
    const response = await anthropic.messages.create({
      model: modelName,
      max_tokens: expert.complexityLevel === 'simple' ? 800 : 
                  expert.complexityLevel === 'medium' ? 1200 : 1600,
      system: expert.systemInstruction,
      messages: anthropicMessages as any,
    });

    const inputTokens = response.usage.input_tokens;
    const outputTokens = response.usage.output_tokens;
    
    const inputCost = (inputTokens / 1_000_000) * modelConfig.input;
    const outputCost = (outputTokens / 1_000_000) * modelConfig.output;
    const totalCost = inputCost + outputCost;

    const content = response.content[0].type === 'text' ? response.content[0].text : '';

    console.log(`âœ… Claude ${modelName} | Cost: $${totalCost.toFixed(5)} | Tokens: ${inputTokens + outputTokens}`);

    return {
      content,
      modelUsed: modelName,
      cost: totalCost,
      tokensUsed: { input: inputTokens, output: outputTokens },
      provider: 'claude',
      cached: false
    };

  } catch (error: any) {
    console.error('âŒ Claude API Error:', error);
    
    // Better error messages
    if (error.status === 401) {
      throw new Error('Ù…ÙØªØ§Ø­ Claude API ØºÙŠØ± ØµØ§Ù„Ø­. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª.');
    } else if (error.status === 429) {
      throw new Error('ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ù…Ù† Ø§Ù„Ø·Ù„Ø¨Ø§Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.');
    } else if (error.status === 500) {
      throw new Error('Ø®Ø·Ø£ ÙÙŠ Ø®Ø§Ø¯Ù… Claude. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.');
    }
    
    throw new Error(`Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Claude: ${error.message}`);
  }
}

// ===== Gemini API =====
async function callGemini(
  messages: ChatMessage[],
  expert: Expert,
  attachments: ChatAttachment[]
): Promise<AIResponse> {
  
  const apiKey = import.meta.env.VITE_GEMINI_API_KEY;
  if (!apiKey) {
    throw new Error('Ù…ÙØªØ§Ø­ Gemini API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©');
  }

  // Use a stable, widely available Gemini model
  const modelName = 'gemini-1.5-flash-8b-latest';
  const modelConfig = MODEL_COSTS['gemini-flash'] || { input: 0, output: 0 };
  
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent`;

  // Build contents: prepend system instruction to ensure compatibility without systemInstruction field
  const contents = [
    {
      role: 'user',
      parts: [{ text: `SYSTEM INSTRUCTIONS:\n${expert.systemInstruction}` }]
    },
    ...messages
      .filter(m => m.role !== 'system')
      .map(msg => ({
        role: msg.role === 'model' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      }))
  ];

  const body = {
    contents,
    generationConfig: {
      temperature: 0.7,
      maxOutputTokens: expert.complexityLevel === 'simple' ? 800 : 
                       expert.complexityLevel === 'medium' ? 1200 : 1600,
      topP: 0.95,
      topK: 40,
    }
  };

  try {
    const response = await fetch(url, {
      method: 'POST',
      headers: { 
        'Content-Type': 'application/json',
        'x-goog-api-key': apiKey,
      },
      body: JSON.stringify(body)
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      console.error('Gemini API Error:', errorData);
      
      if (response.status === 400) {
        const msg = (errorData && (errorData.error?.message || errorData.message)) || 'Ø·Ù„Ø¨ ØºÙŠØ± ØµØ§Ù„Ø­ Ø¥Ù„Ù‰ Gemini API';
        throw new Error(msg);
      } else if (response.status === 401 || response.status === 403) {
        throw new Error('Ù…ÙØªØ§Ø­ Gemini API ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©');
      } else if (response.status === 429) {
        throw new Error('ØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­. Gemini Ù…Ø¬Ø§Ù†ÙŠ Ø­ØªÙ‰ 1500 Ø·Ù„Ø¨/ÙŠÙˆÙ…');
      }
      
      throw new Error(`Ø®Ø·Ø£ Ù…Ù† Gemini: ${response.status}`);
    }

    const data = await response.json();
    
    // Extract content
    const content = data.candidates?.[0]?.content?.parts?.[0]?.text || 
                    data.candidates?.[0]?.text || "";
    
    if (!content) {
      throw new Error('Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Gemini');
    }

    // Calculate cost (Gemini Flash is free up to 1500 requests/day)
    const usage = data.usageMetadata || { promptTokenCount: 0, candidatesTokenCount: 0 };
    const inputTokens = usage.promptTokenCount || 0;
    const outputTokens = usage.candidatesTokenCount || 0;

    // Gemini Flash is free for now
    const inputCost = (inputTokens / 1_000_000) * modelConfig.input;
    const outputCost = (outputTokens / 1_000_000) * modelConfig.output;
    const totalCost = inputCost + outputCost;

    console.log(`âœ¨ Gemini ${modelName} | Cost: $${totalCost.toFixed(5)} (Free tier) | Tokens: ${inputTokens + outputTokens}`);

    return {
      content,
      modelUsed: modelName,
      cost: totalCost,
      tokensUsed: { input: inputTokens, output: outputTokens },
      provider: 'gemini',
      cached: false
    };

  } catch (error: any) {
    console.error('âŒ Gemini API Error:', error);
    
    // If it's already a formatted error, throw it
    if (error.message.includes('Ù…ÙØªØ§Ø­') || error.message.includes('ØªØ¬Ø§ÙˆØ²')) {
      throw error;
    }
    
    throw new Error(`Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Gemini: ${error.message}`);
  }
}

// ===== Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ­Ø¯Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© =====
export const callUnifiedAPI = async (
  messages: ChatMessage[],
  expert: Expert,
  attachments: ChatAttachment[] = []
): Promise<AIResponse> => {
  
  // Get the last user message for cache key
  const lastUserMessage = [...messages].reverse().find(m => m.role === 'user');
  if (!lastUserMessage) {
    throw new Error('Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…');
  }

  // 1. Check cache first
  const cached = responseCache.get(expert.id, lastUserMessage.content);
  if (cached) {
    console.log('ğŸ¯ Cache Hit - Zero Cost!');
    
    const cachedResponse: AIResponse = {
      content: cached.response,
      modelUsed: 'cached',
      cost: 0, // No cost for cached responses
      tokensUsed: { input: 0, output: 0 },
      provider: expert.apiProvider,
      cached: true
    };
    
    usageTracker.trackCall(cachedResponse);
    return cachedResponse;
  }

  // 2. Make API call
  let response: AIResponse;

  try {
    if (expert.apiProvider === 'claude') {
      response = await callClaude(messages, expert, attachments);
    } else if (expert.apiProvider === 'gemini') {
      response = await callGemini(messages, expert, attachments);
    } else {
      throw new Error(`Ù…Ø²ÙˆØ¯ API ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: ${expert.apiProvider}`);
    }

    // 3. Cache the response
    responseCache.set(expert.id, lastUserMessage.content, response.content, response.cost);

    // 4. Track usage
    usageTracker.trackCall(response);

    return response;

  } catch (error: any) {
    console.error('âŒ API Error:', error);
    throw error;
  }
};

// ===== Ø¯Ø§Ù„Ø© ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªÙƒÙ„ÙØ© =====
export const estimateCost = (
  expert: Expert,
  questionLength: number,
  expectedResponseLength: number = 400
): { model: string; estimatedCost: number; provider: string } => {
  
  const modelConfig = MODEL_COSTS[expert.recommendedModel as keyof typeof MODEL_COSTS];
  
  if (!modelConfig) {
    return {
      model: expert.recommendedModel,
      estimatedCost: 0,
      provider: expert.apiProvider
    };
  }

  // Rough token estimation (1 token â‰ˆ 0.75 Arabic words)
  const inputTokens = Math.ceil((questionLength + expert.systemInstruction.length) / 0.75);
  const outputTokens = Math.ceil(expectedResponseLength / 0.75);
  
  const inputCost = (inputTokens / 1_000_000) * modelConfig.input;
  const outputCost = (outputTokens / 1_000_000) * modelConfig.output;
  
  return {
    model: expert.recommendedModel,
    estimatedCost: inputCost + outputCost,
    provider: expert.apiProvider
  };
};

// ===== ØªØµØ¯ÙŠØ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØ§Ø´ =====
export const getCacheStats = () => responseCache.getStats();

// ===== Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´ ÙŠØ¯ÙˆÙŠØ§Ù‹ =====
export const clearCache = () => {
  localStorage.removeItem('ai_response_cache');
  console.log('ğŸ§¹ Cache cleared manually');
};
